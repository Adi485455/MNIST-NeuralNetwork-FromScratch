# MNIST Neural Network from Scratch

This project is a **neural network built from scratch in Python using NumPy** to classify handwritten digits from the **MNIST dataset**. It demonstrates the core concepts of deep learning, including **forward and backward propagation**, **gradient descent**, and **one-hot encoding**, without using high-level libraries like TensorFlow or PyTorch.

## Project Highlights

- Implemented **forward propagation** and **backward propagation** manually.
- Built **gradient descent** to update network weights and biases.
- Used **one-hot encoding** and **data normalization** for better training performance.
- Achieved **~85% accuracy** on the MNIST training set.
- Visualized predictions for individual digits using **Matplotlib**.
- Provides a clear understanding of **how neural networks learn** from scratch.

## Dataset

- Uses the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), which contains **70,000 handwritten digit images** (28x28 pixels):
  - 60,000 training images
  - 10,000 test images

## Usage

1. Clone the repository:
   ```bash
   git clone 
   2.	Open the Jupyter notebook (.ipynb) in your preferred environment (Jupyter Lab, VS Code, Google Colab).
	3.	Run all cells to train the network and visualize predictions.

Requirements
	•	Python 3.x
	•	NumPy
	•	Matplotlib

Visuals
You can visualize the network predictions for individual digits. Example:
Prediction: 5
Label: 5

Author

Aditya Jalindar Turkunde – Second-year CSE student passionate about Machine Learning and Deep Learning fundamentals.
GitHub: 
LinkedIn: 


License

This project is licensed under the MIT License – see the LICENSE file for details.





